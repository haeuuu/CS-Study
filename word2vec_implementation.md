### negative sampling을 매번 하도록 구현해야 할까?

나는 `[center, contexts]` 마다 `multinomial dist` 에서 k개씩 뽑도록 짰다. ( data 수만큼 sampling 해야한다. )

참고한 코드에는 미리 만 개를 뽑아놓고 앞에서부터 차례로 5개씩 나눠주도록 짰다. (약 data // 10000개 만큼 sampling해야한다.)



시간 복잡도를 생각하기 전에 두 방법을 비교해보자.

(k+1)개의 단어가 있고 각 단어가 뽑힐 확률 pi가 주어져있다.

이를 이용해서 5000개의 sample을 구성하려고 한다. x~k+1~ 번째 단어가 등장하는 빈도의 기댓값은 다를까?

1. 5개씩 sampling하는 시행을 1000번 한다. (각 시행은 독립이다.)
   * multinomial(n = 5, (p~1~ , ... , p~k+1~ )) 을 1000번 한다.
   * 나머지 단어가 어떻게 뽑히든지 우리가 원하는 x~k+1~ 만 x개가 뽑히면 된다 !!
   * E(X~k+1~) = 5*p~k+1~ 
   * Y = 1000*X이므로 (indep) E(Y) = 5000 *p~k+1~ 
2. 미리 2500개씩 뽑아놓고 차례로 5개씩 분배한다.
   * multinomial(n = 2500, (p~1~ , ... , p~k+1~ )) 을 2번 한다.
   * 역시 나머지 단어가 몇 개 씩 뽑히는지는 신경쓰지 않아도 된다.
   * 각 시행시마다 E(X~k+1~) = 2500*p~k+1~ 
   * Y = 2*X이므로 5000 *p~k+1~ 



결론은, 모든 X~i~에 대해 기댓값이 같다 !!!



그럼 다시 시간 복잡도를 생각해보자.
